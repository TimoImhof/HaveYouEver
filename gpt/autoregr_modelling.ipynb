{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content of this file based on [Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)'s youtube video on autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  3 1089k    3 39962    0     0   182k      0  0:00:05 --:--:--  0:00:05  183k\n",
      "100 1089k  100 1089k    0     0  3354k      0 --:--:-- --:--:-- --:--:-- 3361k\n"
     ]
    }
   ],
   "source": [
    "# Download dataset to train on\n",
    "!curl.exe --output shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# unique chars \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create in a mapping which will be used as a dummy tokenizer for character level encoding\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# We can now encode the whole dataset and store it in a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# The tensor will be useful for random sampling during training and also for creating train and test split\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive training and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) try to predict: 47\n",
      "when input is tensor([18, 47]) try to predict: 56\n",
      "when input is tensor([18, 47, 56]) try to predict: 57\n",
      "when input is tensor([18, 47, 56, 57]) try to predict: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) try to predict: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) try to predict: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) try to predict: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) try to predict: 58\n"
     ]
    }
   ],
   "source": [
    "# This code block demonstrates what the model actually learns during training\n",
    "#  - given a set of characters as input, the model will learn to predict the characters that come afterwards\n",
    "#  => it learns to generate new text given a text input (hurray!)\n",
    "\n",
    "# The block_size determines how many text units (characters in our case) are passed in one sample, e.g. 8 in this case\n",
    "block_size = 8\n",
    "\n",
    "# We can use one sample to actually generate n = block_size - 1 samples for training\n",
    "# (This way the model learns to generate text given variable sized inputs)\n",
    "\n",
    "def describe_sample(x,y):\n",
    "    for t in range(block_size):\n",
    "        context = x[:t+1]\n",
    "        target = y[t]\n",
    "        print(f\"when input is {context} try to predict: {target}\")\n",
    "        \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "describe_sample(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematically formulated we are trying to maximize the likelihood of the observed sequences in the training data:\n",
    "# P(x_1, x_2, ..., x_n) = P(x_1) * P(x_2|x_1) * P(x_3|x_1, x_2) * ... * P(x_n|x_1, ..., x_{n-1})\n",
    "\n",
    "# For optimization purposes (log numerically more stable + TODO: come up with better explanation) we use the negative log-likelihod:\n",
    "# -log(P(x_1,x_2,...,x_n)) = -\\sum_i log(P(x_i|x_1, ..., x_{i-1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training we actually don't just want to pass a single sample, but train with multiple samples in parallel\n",
    "# This will speed up training tremendously because we (when given) leverage the capabilities of gpu to process data in parallel\n",
    "# In parallel does not mean that the samples interact with each other, it is just fast but in essence the same intuition as passing all samples iteratively\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample number: 0\n",
      "when input is tensor([6]) try to predict: 1\n",
      "when input is tensor([6, 1]) try to predict: 52\n",
      "when input is tensor([ 6,  1, 52]) try to predict: 53\n",
      "when input is tensor([ 6,  1, 52, 53]) try to predict: 58\n",
      "when input is tensor([ 6,  1, 52, 53, 58]) try to predict: 1\n",
      "when input is tensor([ 6,  1, 52, 53, 58,  1]) try to predict: 58\n",
      "when input is tensor([ 6,  1, 52, 53, 58,  1, 58]) try to predict: 47\n",
      "when input is tensor([ 6,  1, 52, 53, 58,  1, 58, 47]) try to predict: 50\n",
      "Sample number: 1\n",
      "when input is tensor([6]) try to predict: 1\n",
      "when input is tensor([6, 1]) try to predict: 54\n",
      "when input is tensor([ 6,  1, 54]) try to predict: 50\n",
      "when input is tensor([ 6,  1, 54, 50]) try to predict: 39\n",
      "when input is tensor([ 6,  1, 54, 50, 39]) try to predict: 52\n",
      "when input is tensor([ 6,  1, 54, 50, 39, 52]) try to predict: 58\n",
      "when input is tensor([ 6,  1, 54, 50, 39, 52, 58]) try to predict: 43\n",
      "when input is tensor([ 6,  1, 54, 50, 39, 52, 58, 43]) try to predict: 58\n",
      "Sample number: 2\n",
      "when input is tensor([1]) try to predict: 58\n",
      "when input is tensor([ 1, 58]) try to predict: 46\n",
      "when input is tensor([ 1, 58, 46]) try to predict: 47\n",
      "when input is tensor([ 1, 58, 46, 47]) try to predict: 57\n",
      "when input is tensor([ 1, 58, 46, 47, 57]) try to predict: 1\n",
      "when input is tensor([ 1, 58, 46, 47, 57,  1]) try to predict: 50\n",
      "when input is tensor([ 1, 58, 46, 47, 57,  1, 50]) try to predict: 47\n",
      "when input is tensor([ 1, 58, 46, 47, 57,  1, 50, 47]) try to predict: 60\n",
      "Sample number: 3\n",
      "when input is tensor([0]) try to predict: 32\n",
      "when input is tensor([ 0, 32]) try to predict: 46\n",
      "when input is tensor([ 0, 32, 46]) try to predict: 43\n",
      "when input is tensor([ 0, 32, 46, 43]) try to predict: 56\n",
      "when input is tensor([ 0, 32, 46, 43, 56]) try to predict: 43\n",
      "when input is tensor([ 0, 32, 46, 43, 56, 43]) try to predict: 1\n",
      "when input is tensor([ 0, 32, 46, 43, 56, 43,  1]) try to predict: 42\n",
      "when input is tensor([ 0, 32, 46, 43, 56, 43,  1, 42]) try to predict: 53\n"
     ]
    }
   ],
   "source": [
    "x_b, y_b = get_batch('train)')\n",
    "\n",
    "# We can check now every sample in the batch as before:\n",
    "for i, (x_s, y_s) in enumerate(zip(x_b, y_b)):\n",
    "    print(f\"Sample number: {i}\")\n",
    "    describe_sample(x_s, y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple bigram language model that demonstrates the core concepts of autoregressive models.\n",
    "# A bigram model predicts the next token based only on the previous token, making it the simplest\n",
    "# form of an autoregressive model.\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "        # The embedding table learns a mapping from each token to a vector of scores.\n",
    "        # For a bigram model, this vector represents the \"logits\" (raw scores) for\n",
    "        # predicting the next token. Shape: (vocab_size, vocab_size)\n",
    "        # Example: row i contains scores for what token should follow token i\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        # inputs: tensor of token indices, shape (B,T) where:\n",
    "        #   B = batch size (number of sequences)\n",
    "        #   T = sequence length (number of tokens per sequence)\n",
    "        # targets: tensor of next-token indices, shape (B,T), shifted one position right\n",
    "        #   For input sequence \"hello\", targets would be \"ello<end>\"\n",
    "        logits = self.token_embedding_table(inputs) # (B,T,C)\n",
    "        # For each position in each sequence, look up the corresponding row in the embedding table\n",
    "        # This gives us scores for what token should follow each input token\n",
    "        # C = vocab_size (number of possible next tokens)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # Reshape logits and targets to treat each position as an independent prediction\n",
    "            logits = logits.view(B*T, C)    # shape: (B*T, C)\n",
    "            targets = targets.view(B*T)      # shape: (B*T)\n",
    "            # Cross entropy loss measures how well our predictions match the actual next tokens\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # Generate new tokens one at a time, using only the last token to predict the next\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(inputs)  # shape: (B,T,C)\n",
    "            # For generation, we only need the predictions from the last position\n",
    "            # since a bigram model only uses the previous token\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)  # shape: (B,C)\n",
    "            \n",
    "            # Sample from the probability distribution to get the next token\n",
    "            inputs_next = torch.multinomial(probs, num_samples=1)  # shape: (B,1)\n",
    "            # Append the new token to our sequence\n",
    "            inputs = torch.cat((inputs, inputs_next), dim=1)  # shape: (B,T+1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JOHulche; h co.\n",
      "CouCKI:\n",
      "LABe y, crd bo tarreror thindrariathitot.\n",
      "Thathest\n",
      "INOWe hin t 's ve het\n",
      "LEL\n"
     ]
    }
   ],
   "source": [
    "# If we now generate text with the untrained model we get glibberish\n",
    "print(decode(m.generate(inputs=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4584102630615234\n"
     ]
    }
   ],
   "source": [
    "# So let's train it and see how the output changes when starting from an empty sequence\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)  # We use a relatively large learning rate, because the model is fairly small\n",
    "batch_size = 32\n",
    "\n",
    "# Train\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MENG y e msbe shes, d th, h youre w ag mur ore irt\n",
      "Ano and t wis, cl\n",
      "\n",
      "Thof ty dsuran n: d athe hor\n",
      "TUncall sprame I INIsatsooruraumendeleave? 'TINGisthe ing'de atioprd at b'ls lalllod ut\n",
      "Orice wau e inor Lishiste,\n",
      "\n",
      "LENTel s, S:\n",
      "Au torsuth or urren tharit amigl macte'd ipr har ircr\n",
      "PEREveay qust ty DWathende IN ndyond toust; canikill, UTO wickes we l f,\n",
      "\n",
      "Yonovey ou Rod wey penmean thie bye hof ARKI:\n",
      "Bu chey whimazes\n",
      "The a m ssorersast CUCE:\n",
      "Twilee nd:\n",
      "Heewisongengay-'l hend, lfofor moue outy:\n",
      "\n",
      "AR\n"
     ]
    }
   ],
   "source": [
    "# It is still glibberish, but not that random, we can see fractions of real language! (yippieh)\n",
    "print(decode(m.generate(inputs = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
