{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content of this file based on [Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)'s youtube video on autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1089k  100 1089k    0     0  6128k      0 --:--:-- --:--:-- --:--:-- 6188k\n",
      "c:\\Users\\timoi\\dev\\miniconda3\\envs\\hye\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Download dataset to train on\n",
    "!curl.exe --output shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# unique chars \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create in a mapping which will be used as a dummy tokenizer for character level encoding\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# We can now encode the whole dataset and store it in a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# The tensor will be useful for random sampling during training and also for creating train and test split\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive training and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) try to predict: 47\n",
      "when input is tensor([18, 47]) try to predict: 56\n",
      "when input is tensor([18, 47, 56]) try to predict: 57\n",
      "when input is tensor([18, 47, 56, 57]) try to predict: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) try to predict: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) try to predict: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) try to predict: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) try to predict: 58\n"
     ]
    }
   ],
   "source": [
    "# This code block demonstrates what the model actually learns during training\n",
    "#  - given a set of characters as input, the model will learn to predict the characters that come afterwards\n",
    "#  => it learns to generate new text given a text input (hurray!)\n",
    "\n",
    "# The block_size determines how many text units (characters in our case) are passed in one sample, e.g. 8 in this case\n",
    "block_size = 8\n",
    "\n",
    "# We can use one sample to actually generate n = block_size - 1 samples for training\n",
    "# (This way the model learns to generate text given variable sized inputs)\n",
    "\n",
    "def describe_sample(x,y):\n",
    "    for t in range(block_size):\n",
    "        context = x[:t+1]\n",
    "        target = y[t]\n",
    "        print(f\"when input is {context} try to predict: {target}\")\n",
    "        \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "describe_sample(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematically formulated we are trying to maximize the likelihood of the observed sequences in the training data:\n",
    "# P(x_1, x_2, ..., x_n) = P(x_1) * P(x_2|x_1) * P(x_3|x_1, x_2) * ... * P(x_n|x_1, ..., x_{n-1})\n",
    "\n",
    "# For optimization purposes (log numerically more stable + TODO: come up with better explanation) we use the negative log-likelihod:\n",
    "# -log(P(x_1,x_2,...,x_n)) = -\\sum_i log(P(x_i|x_1, ..., x_{i-1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training we actually don't just want to pass a single sample, but train with multiple samples in parallel\n",
    "# This will speed up training tremendously because we (when given) leverage the capabilities of gpu to process data in parallel\n",
    "# In parallel does not mean that the samples interact with each other, it is just fast but in essence the same intuition as passing all samples iteratively\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample number: 0\n",
      "when input is tensor([1]) try to predict: 47\n",
      "when input is tensor([ 1, 47]) try to predict: 57\n",
      "when input is tensor([ 1, 47, 57]) try to predict: 1\n",
      "when input is tensor([ 1, 47, 57,  1]) try to predict: 58\n",
      "when input is tensor([ 1, 47, 57,  1, 58]) try to predict: 46\n",
      "when input is tensor([ 1, 47, 57,  1, 58, 46]) try to predict: 43\n",
      "when input is tensor([ 1, 47, 57,  1, 58, 46, 43]) try to predict: 1\n",
      "when input is tensor([ 1, 47, 57,  1, 58, 46, 43,  1]) try to predict: 44\n",
      "Sample number: 1\n",
      "when input is tensor([0]) try to predict: 32\n",
      "when input is tensor([ 0, 32]) try to predict: 46\n",
      "when input is tensor([ 0, 32, 46]) try to predict: 47\n",
      "when input is tensor([ 0, 32, 46, 47]) try to predict: 57\n",
      "when input is tensor([ 0, 32, 46, 47, 57]) try to predict: 1\n",
      "when input is tensor([ 0, 32, 46, 47, 57,  1]) try to predict: 44\n",
      "when input is tensor([ 0, 32, 46, 47, 57,  1, 44]) try to predict: 39\n",
      "when input is tensor([ 0, 32, 46, 47, 57,  1, 44, 39]) try to predict: 60\n",
      "Sample number: 2\n",
      "when input is tensor([58]) try to predict: 61\n",
      "when input is tensor([58, 61]) try to predict: 39\n",
      "when input is tensor([58, 61, 39]) try to predict: 56\n",
      "when input is tensor([58, 61, 39, 56]) try to predict: 42\n",
      "when input is tensor([58, 61, 39, 56, 42]) try to predict: 1\n",
      "when input is tensor([58, 61, 39, 56, 42,  1]) try to predict: 44\n",
      "when input is tensor([58, 61, 39, 56, 42,  1, 44]) try to predict: 39\n",
      "when input is tensor([58, 61, 39, 56, 42,  1, 44, 39]) try to predict: 41\n",
      "Sample number: 3\n",
      "when input is tensor([27]) try to predict: 10\n",
      "when input is tensor([27, 10]) try to predict: 0\n",
      "when input is tensor([27, 10,  0]) try to predict: 35\n",
      "when input is tensor([27, 10,  0, 35]) try to predict: 46\n",
      "when input is tensor([27, 10,  0, 35, 46]) try to predict: 63\n",
      "when input is tensor([27, 10,  0, 35, 46, 63]) try to predict: 6\n",
      "when input is tensor([27, 10,  0, 35, 46, 63,  6]) try to predict: 1\n",
      "when input is tensor([27, 10,  0, 35, 46, 63,  6,  1]) try to predict: 28\n"
     ]
    }
   ],
   "source": [
    "x_b, y_b = get_batch('train)')\n",
    "\n",
    "# We can check now every sample in the batch as before:\n",
    "for i, (x_s, y_s) in enumerate(zip(x_b, y_b)):\n",
    "    print(f\"Sample number: {i}\")\n",
    "    describe_sample(x_s, y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mr. Karpathy built this simple autoregressive language model, which is completely sufficient to explain the point of this\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "        # nn.Embedding is in essence just a minimal wrapper around a tensor in our case of size vocab_size x vocab_size\n",
    "        # and when forwardinga number through the embedding we essentially return the ith vector of the embedding tensor\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        \n",
    "        # inputs and targets are both tensors of shape (batch_size/B, num_characters/T)\n",
    "        # Mr. Karpathy revered to num_characters also as the time dimension T, which is also a very intuitive way of thinking about this\n",
    "        logits = self.token_embedding_table(inputs) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # stretches out the second dimension, required for input format of cross_entropy\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(inputs) # (B,T, C)\n",
    "            # In the Bigram model we are only interested in the last time step \n",
    "            # (We perform our prediction of the next character only based on the character that is directly preceding, hence \"bi\"-gram model)\n",
    "            # We apply softmax to transform logits into probabilities (so the values sum up to one)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            \n",
    "            # Then sample from the probability distribution\n",
    "            inputs_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to running sequence\n",
    "            inputs = torch.cat((inputs,inputs_next), dim=1) # (B, T+1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([32, 65])\n",
      "tensor([[-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [-0.1679,  0.5602,  0.6467,  ...,  0.1522,  0.5109,  0.0990],\n",
      "        ...,\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47]])\n",
      "tensor(4.4150, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(yb.shape)\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "print(yb)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5727508068084717\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers je are!-e!\n",
      "QLYotouciullle'z\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
