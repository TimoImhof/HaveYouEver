{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_embd = 32\n",
    "batch_size = 16\n",
    "block_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify our simple bigram model to a simple attention model \n",
    "# The model uses an attention head, which was derived in part 2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    " \n",
    "# Implementation of a single attention head   \n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key_repr = nn.Linear(n_embd, block_size, bias=False)\n",
    "        self.query_repr = nn.Linear(n_embd, block_size, bias=False)\n",
    "        self.value_repr = nn.Linear(n_embd, block_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # create learned representations\n",
    "        k = self.key_repr(x)\n",
    "        q = self.query_repr(x)\n",
    "        v = self.value_repr(x)\n",
    "        \n",
    "        # compute attention scores ('affinities between tokens')\n",
    "        W = q @ k.transpose(-2,-1)  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        W *= C ** -0.5  # scaling of the dot product to keep softmax from saturating too much\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B,T,T) # TODO: explain [:T, :T]\n",
    "        W = F.softmax(W, dim=-1) # (B,T,T)\n",
    "        \n",
    "        # compute weighted aggregation of values\n",
    "        out = W @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the new model that modifies the Bigram model to use an attention head.\n",
    "# For this we now also modify the embedding of the input:\n",
    "# We use an additional positional encoding to add information about the temporal dimension of each token.\n",
    "# This is useful because attention in itself is a position agnostic communication mechanism, where the position/time is not taken into account by nature.\n",
    "\n",
    "# In addition to the self-attention head, we also add a final linear layer, which is called lm_head. \n",
    "# Here we project our output back to the dimension of the vocabulary so we can compute a probability distribution of all characters in the vocabulary.\n",
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embeddings_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd=n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs: torch.tensor, targets: torch.tensor = None):\n",
    "        B, T = inputs.shape\n",
    "        \n",
    "        # inputs and targets are both (B,T) tensors of integers\n",
    "        tok_emb = self.token_embeddings_table(inputs)\n",
    "        pos_emb = self.position_embeddings_table(torch.arange(T)) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x)  # apply one head of self-attention (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # Generate new tokens one at a time, \n",
    "        # using now the full sequence of n = block_size tokens, weighted via self-attention,\n",
    "        # to decide on the next generated token\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # make sure the inputs are at max block_size number of tokens\n",
    "            # necessary because our position embedding can only encode position information for up to block_size tokens\n",
    "            model_inputs = inputs[:, -block_size:]\n",
    "            #print(f\"Generate: inputs.shape = {inputs.shape}\")\n",
    "            \n",
    "            logits, _ = self(model_inputs)  # shape: (B,T,C)\n",
    "            # For generation, we only need the predictions from the last position\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)  # shape: (B,C)\n",
    "            \n",
    "            # Sample from the probability distribution to get the next token\n",
    "            inputs_next = torch.multinomial(probs, num_samples=1)  # shape: (B,1)\n",
    "            \n",
    "            # Append the new token to our sequence\n",
    "            inputs = torch.cat((inputs, inputs_next), dim=1)  # shape: (B,T+1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# This cell adds context form part 1 so that we can perform a similar training run as for the bigram model.\n",
    "\n",
    "# Read dataset\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# How we encode text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Dataset encoding\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Generating train and test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Generate random batch from the training split\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".gRIuavUDT E:W KyHiHS\n",
      "hY cEKmNeg3daaSGA.nQWSfQXuww$y;lQZUI\n",
      "h:E-NSzUDZIjlFy$vIPC&?cwO&S,gv\n",
      " kh\n",
      "\n",
      "VwiwMt!QWAZKn&L:J'.u;MOBrLm-:C3mAKOngcvn\n",
      "WoVVQ!iDuKt3YK$rOGcGTKAIJupIO\n",
      "GtOfGlEpT,;CxER$lEVJ&ewZ?BknZ-\n",
      "hf:$ \n",
      "hFSzDhAc3'?DHWsV,L.XrXD.IuY3.3aWF,3VEG\n",
      "&z!NorVa'mXMXSXZLFaq'ZImxYAMahxZGbKTin',Id3doCI;F\n",
      "vuDcFipg,C,:g;3FminQxS'anNcZwVy;fx&;NXJ-\n",
      "f3R;i\n",
      ",aaUY\n",
      " dF'j$lK-3Wbs;&jhWDccwCci&FFZ\n",
      "jticGBJPNAQciekv3dwfD&UE$zHAEMvq3;YZDYJ.!R\n",
      "bqDRcGsose'g,Y'Qf.vYJ;pbkXMNI,cqbgEhBnrnHyN$gC'XHs\n",
      "ElGJ,.z\n",
      "pYE?rmyXQ:LpkzUIuLsBhlB\n"
     ]
    }
   ],
   "source": [
    "model = SimpleAttentionModel(n_embd=n_embd, vocab_size=vocab_size)\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.442214250564575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we train our simple attention head model!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "# We can see that, compared to our bigram model loss with rougly 2.57,\n",
    "# we can achieve a smaller loss of 2.39 with the same amount of training steps!\n",
    "# This is a nice improvement, however, if we generate now text with our trained model, \n",
    "# it is still not the yellow from the egg (german joke sry O_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "Why hary, omolurs itheande farr\n",
      "'CHHien los hfid.\n",
      "QUE:\n",
      "Lostlo meaghorours\n",
      "Fo sheras thasepies tava pu, wire dy I to, ge whe wred\n",
      "T:\n",
      "Hot!\n",
      "SI thre sirsomeard, ofurd frak vexser eet thifr.\n",
      "BRGAndld bere Rinoto's faly stem meakeet bre theexoceracmat Ork athe\n",
      "JLETS:\n",
      "O this.\n",
      "\n",
      "Win blitilelyouslow hy ce\n",
      "thangad ary yourf houin, kis farrt thit chas stsothite sefr f heargeame:\n",
      "Pat un limit mones tit oter anst, lsives, shor aur?\n",
      "\n",
      "Cormifir's t thiver adne shos fig myou peresworb,\n",
      "NGrd woome.\n",
      "NORWIIIS:\n",
      "Horag\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
