{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training a Model with Multi-Head Attention\n",
    "\n",
    "In this chapter we built upon our derivation of a single attention head:\n",
    "\n",
    "- We create an config class to store hyperparameters\n",
    "- We create an Single Attention Head class\n",
    "- We build an attention head into the model and adapt the `generate` and `forward` functions of the model\n",
    "- We expand the single attention head model to a multi-head attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For storing and managing hyperparameters we create a config class so that all parameters easily accessible in one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 65,\n",
    "                 n_embd: int = 32,\n",
    "                 block_size: int = 32,\n",
    "                 num_heads: int = 1,\n",
    "                 batch_size = 16\n",
    "                 ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Part 2 implement our derived attention mechansims bundled in one attention head class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify our simple bigram model to a simple attention model \n",
    "# The model uses an attention head, which was derived in part 2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    " \n",
    "# Implementation of a single attention head   \n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single Attention Head\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.query_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.value_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        # Register buffer for attention mask\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # create learned representations\n",
    "        k = self.key_repr(x)\n",
    "        q = self.query_repr(x)\n",
    "        v = self.value_repr(x)\n",
    "        \n",
    "        # compute attention scores ('affinities between tokens')\n",
    "        W = q @ k.transpose(-2,-1)  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        W *= C ** -0.5  # scaling of the dot product to keep softmax from saturating too much\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B,T,T) # TODO: explain [:T, :T]\n",
    "        W = F.softmax(W, dim=-1) # (B,T,T)\n",
    "        \n",
    "        # compute weighted aggregation of values\n",
    "        out = W @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we now can augment our Bigram model and use an attention head to use the information of a the complete sequence to predict the next token instead of only the most recent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this we now also modify the embedding of the input:\n",
    "# We use an additional positional encoding to add information about the temporal dimension of each token.\n",
    "# This is useful because attention in itself is a position agnostic communication mechanism, where the position/time is not taken into account by nature.\n",
    "\n",
    "# In addition to the self-attention head, we also add a final linear layer, which is called lm_head. \n",
    "# Here we project our output back to the dimension of the vocabulary so we can compute a probability distribution of all characters in the vocabulary.\n",
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embeddings_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.sa_head = Head(config=config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "    def forward(self, inputs: torch.tensor, targets: torch.tensor = None):\n",
    "        B, T = inputs.shape\n",
    "        \n",
    "        # inputs and targets are both (B,T) tensors of integers\n",
    "        tok_emb = self.token_embeddings_table(inputs)\n",
    "        pos_emb = self.position_embeddings_table(torch.arange(T)) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x)  # apply one head of self-attention (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # Generate new tokens one at a time, \n",
    "        # using now the full sequence of n = block_size tokens, weighted via self-attention,\n",
    "        # to decide on the next generated token\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # make sure the inputs are at max block_size number of tokens\n",
    "            # necessary because our position embedding can only encode position information for up to block_size tokens\n",
    "            model_inputs = inputs[:, -self.block_size:]\n",
    "            #print(f\"Generate: inputs.shape = {inputs.shape}\")\n",
    "            \n",
    "            logits, _ = self(model_inputs)  # shape: (B,T,C)\n",
    "            # For generation, we only need the predictions from the last position\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)  # shape: (B,C)\n",
    "            \n",
    "            # Sample from the probability distribution to get the next token\n",
    "            inputs_next = torch.multinomial(probs, num_samples=1)  # shape: (B,1)\n",
    "            \n",
    "            # Append the new token to our sequence\n",
    "            inputs = torch.cat((inputs, inputs_next), dim=1)  # shape: (B,T+1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of single attention head model\n",
    "\n",
    "Now if we train the model unlike before we will learn now the affinities between tokens, which should result in better model performance than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# This cell adds context form part 1 so that we can perform a similar training run as for the bigram model.\n",
    "\n",
    "# Read dataset\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# How we encode text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Dataset encoding\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Generating train and test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Generate random batch from the training split\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - config.block_size, size=(config.batch_size,))\n",
    "    x = torch.stack([data[i:i + config.block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+config.block_size+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DVjnTECva?Hhb,UtXzUKPwsIzbzPC'qNi:HxgdvgQsvqTQLnaSoPVt'CVNQ?mUcQturygSPLL!$G!yUO;kaag.BABlZXhsnU&h?GzybTJxnCD,tKzTF.!Kf!L.sR?Wwwxrz-GDVyNFqkMXGojcZCKVTszL,L3SXo!X?SyzU:nfb-\n",
      ";HiLMY&MSYIt&aKySluW\n",
      "-Hghf??eN3NzIznZEzovK3\n",
      "!&bilA,wjh'izmwh?EdOkKnFNE,jpCYjW.tt'yspzADffWHG-eSfjWk:3ybSmEJeXeWBvYwnbPrCmKWsYSZnRdKpSlOMxAU tAm SW.FVDjmTLGz?PVfK?dk\n",
      "zhxNrgBjYgxN;QgHtH ;xrHAeTlwy!LMD$yNjK?\n",
      "Pj3X$XTKZKTh pciO.jmVnJEwwAQgHk&XpUW3K&DjtfI,L\n",
      "INwMezmnGWVJKMdoCdjSIKLrIgmA ltKNmHeG;&3pnh DFeFCMu,IPJ3-i:cZjtxiR.'kMJr3T:\n"
     ]
    }
   ],
   "source": [
    "model = SimpleAttentionModel(config=config)\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.371469497680664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we train our simple attention head model!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, compared to our bigram model loss with rougly 2.57, we can achieve a smaller loss with the same amount of training steps!\n",
    "This is a nice improvement; learning the affinities does bring additional value.\n",
    "However, if we generate now text with our trained model it is still not readable Shakespeare, so there is room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "Yo ma nllia ftitre he thache than beenig;\n",
      "Bury thay at dat sens houthe weral fail thre omarth indrousr; woth thourk thitork, whofur ill gnd nethey thidefatond tho oerd: thug his amy tohus nof fl, the ad thitachis,\n",
      "Theatolo, ot yofrr loren lod tsenco; suisllf GRIITOLeer! heixcofr fret.\n",
      "\n",
      "KI our,\n",
      "GRer, saly bthit, rorwe maud ne yit anve.\n",
      "\n",
      "GRS: nd saighich hart ilsat om?\n",
      "Coprph fimas den icut:\n",
      "YYour ienid.\n",
      "\n",
      "LARour, myou, ma thinerere; sher ove, I wod ble\n",
      "I nd dy:\n",
      "Cobe thalie, be;\n",
      "PUS:\n",
      "He, my, la men\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand to Multi-Head Attention\n",
    "\n",
    "Instead of using only one attention head we can use multiple in parallel. The idea here is that each attention head \"attends\" to a different suspace of the input.\n",
    "Then the output of all the attention heads is concatenated and the joined output should be more comprehensive than the output of a single attention-head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's try using Multi-head attention. Here the input is now only passed through one attention head, but multiple ones in parallel.\n",
    "# The ouput is then concatenated and has the same final dimension (B,T,C) as before.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # During the forwad pass the input is passed in parallel through all the heads and afterwards is concatenated\n",
    "        # To make sure that the concatenated output has the correct dimension, each of the works in a subspace of head_size = n_embd // num_heads\n",
    "        return torch.cat([h(input) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "\n",
    "# In the multi-head attention model we replace the single attention module with the multi-head module.\n",
    "# The residual model stays the same\n",
    "class MultiHeadAttentionModel(SimpleAttentionModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.sa_head = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = MultiHeadAttentionModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6412746906280518\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(multi_model.parameters(), lr=1e-3)\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = multi_model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "mMinTortt ntowe wheour, l, cen tteise bois.\n",
      "Car horl gheome an?\n",
      "Tthisses.\n",
      "\n",
      "\n",
      "Bod chou wauverhit tar:\n",
      "\n",
      "O fs aave, imjattouquro wathe har avend\n",
      "Arsend s d'swe bd chonthigawsbar h we lounounthr ghlingbu.\n",
      "\n",
      "N p\n",
      "\n",
      "\n",
      "VHowirrs lind te t'\n",
      "LAthod CUSo tot ad w fe\n",
      "TAnond;\n",
      "OThimder an ollythe:\n",
      "An e G y,\n",
      "RHof, oluvit pe matord anowar vie, bor bamed wor:\n",
      "EF: Gpun! us be m?\n",
      "LEVICORCis erthoir,\n",
      "SE: sev a gruns thouseapue helgth,\n",
      "I.\n",
      "S'le.\n",
      "MLo, pludinaref 'Flellt pys sin Whet w d l, averyoom h towarlen eris tim! mon\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(multi_model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)\n",
    "\n",
    "# And with that we have derived the Multi-head self-attention mechanism from scratch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
