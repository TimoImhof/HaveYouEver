{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training a Model with Multi-Head Attention\n",
    "\n",
    "In part 2 we derived the code and intutition behind self-attention for a single attention head.\n",
    "In this last chapter we built upon this to do the following:\n",
    "\n",
    "- Augment the bigram model to use a single Attention Head\n",
    "- Expand the single attention head model to a multi-head attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, for storing and managing hyperparameters let's create a dummy config class so that all parameters are easily accessible in one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 65,\n",
    "                 n_embd: int = 32,\n",
    "                 block_size: int = 32,\n",
    "                 num_heads: int = 1,\n",
    "                 batch_size = 16\n",
    "                 ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Bigram Model\n",
    "\n",
    "Now let's take our simple bigram model form the first part and transform into a more capable model that can utilize the full context by using a self-attention head.\n",
    "\n",
    "For this we implement the following components in the following:\n",
    "- A class for a single attention head, which contains the logic derived in part 2\n",
    "- The modified Bigram Model class with:\n",
    "    - **Self-attention head**\n",
    "    - **Positional token embedding** (to introduce information about temporal dimension)\n",
    "    - **Language modeling head** (additional final linear layer to project our output back to the dimension of the vocabulary in order to compute a probability distribution of all characters in the vocabulary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single Attention Head\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.query_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.value_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        # Register buffer for attention mask\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # create learned representations\n",
    "        k = self.key_repr(x)\n",
    "        q = self.query_repr(x)\n",
    "        v = self.value_repr(x)\n",
    "        \n",
    "        # compute attention scores ('affinities between tokens')\n",
    "        W = q @ k.transpose(-2,-1)  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        W *= C ** -0.5  # scaling of the dot product to keep softmax from saturating too much\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B,T,T), we use :T here to make sure we never exceed the context window\n",
    "        W = F.softmax(W, dim=-1) # (B,T,T)\n",
    "        \n",
    "        # compute weighted aggregation of values\n",
    "        out = W @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embeddings_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.sa_head = Head(config=config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, inputs: torch.tensor, targets: torch.tensor = None):\n",
    "        \"\"\" Forward pass of the model where we compute the raw preferences for what to be the next characters. \"\"\"\n",
    "        B, T = inputs.shape\n",
    "        \n",
    "        # inputs and targets are both (B,T) tensors of integers\n",
    "        tok_emb = self.token_embeddings_table(inputs)\n",
    "        pos_emb = self.position_embeddings_table(torch.arange(T)) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x)  # apply one head of self-attention (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        \"\"\" Autoregressive langauge generation function, where based on a given start input we run a forward pass, \n",
    "        select the most probable next token, append it, and repeat the process max_new_tokens times. \"\"\"       \n",
    "        for _ in range(max_new_tokens):\n",
    "            # make sure the inputs are at max block_size number of tokens\n",
    "            # necessary because our position embedding can only encode position information for up to block_size tokens\n",
    "            model_inputs = inputs[:, -self.config.block_size:]\n",
    "            #print(f\"Generate: inputs.shape = {inputs.shape}\")\n",
    "            logits, _ = self(model_inputs)  # shape: (B,T,C)\n",
    "            # For generation, we only need the predictions from the last position\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)  # shape: (B,C)\n",
    "            # Sample from the probability distribution to get the next token\n",
    "            inputs_next = torch.multinomial(probs, num_samples=1)  # shape: (B,1)\n",
    "            # Append the new token to our sequence\n",
    "            inputs = torch.cat((inputs, inputs_next), dim=1)  # shape: (B,T+1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Single Attention Head Model\n",
    "\n",
    "Now we can train our more capable model. Unlike before we will now **learn the affinities between tokens**, which should result in better model performance than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one last time, let's reuse the dataset, encoder/decoder and `get_batch` function form part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1089k  100 1089k    0     0  4437k      0 --:--:-- --:--:-- --:--:-- 4464k\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get dataset\n",
    "!curl.exe --output shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# Create encoder and encode datasetchars = sorted(list(set(text)))\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Generating train and test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - config.block_size, size=(config.batch_size,))\n",
    "    x = torch.stack([data[i:i + config.block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+config.block_size+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our model and as before check what the untrained output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "j?vVOEQvMF-wXabyhCkpFww\n",
      ".wIYwDcqAw-HXygXR$3OkWzxvSOH Zp;QZJg-BHm!kzlxQnSTrto cajDG3PmYXaZis\n",
      "Jpz$nyZc!muYICgZ C3QBI:OVynfvMdMMgGk;RuebBuvK,:avxSvauFL:3RSPKafUQyfNkYHgDgCLU.Abfq'3h3tzCEw?$:mbA?W&!rFvYQb.c3O&b BPh;YiKYyT\n",
      "hJnhh3QhK ZibtgnwNup?enzRuYwiLEKBPXz$VC'qBbQ3&!e.bAF:WdRKrkTlk\n",
      "WdFMJqmbhDr!YCD Gzsys:zKRj .Dsdt tTgO'bov$po$raxDmx;e$3sCXqCs bj;I.-qWbeFV,:anA.-xbo;mCVtXxTEeaYCdO-h3:qDk?BH\n",
      "FZjrcTbVpwTLN?rLFzXdV$k$'E-Tap!hH BhtuexSSS3U\n",
      "Qui!G3nZ3mFKaDllY:JMSlr.\n",
      "HiGxz\n",
      "WeSrzE,m?3TfNBQBMSx?KDGt\n",
      "RqRc\n",
      "&l\n"
     ]
    }
   ],
   "source": [
    "model = SimpleAttentionModel(config=config)\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it is pure gibberish, so now let's train it and compare the loss to the loss of the bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 473.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3612353801727295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Now we train our simple attention head model!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for steps in tqdm(range(10000)): # increase number of steps for good results...\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, compared to our bigram model loss with rougly 2.57, we can achieve a smaller loss with the same amount of training steps!\n",
    "This is a nice improvement; learning the affinities does bring additional value.\n",
    "Of course we performed not enough training to get perfect shakespeare, but fragments of language can now already be observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "AUCNIO:\n",
      "FRMO:\n",
      "Sate't tlan thak as tere my tof tinoer,\n",
      "NORome at,\n",
      "Whabmaref at umpacus than befer\n",
      "VIOPu?\n",
      "Shal\n",
      "Tif than followre boly a thebr's; tour thy ipst, to whald:\n",
      "At mongl-\n",
      "t can\n",
      "dos dors, Rant,\n",
      "R ARWICHery hagh ronget an\n",
      "Whig th astt, mse wrure thath go cow thize igt hlfy, boum owreel the; omonoot; the do bout foleas.\n",
      "\n",
      "Mat mind,\n",
      "Yo foagem io ome be'de's whanwo yon soso rime hsan's the, wo ous LTo CHES:\n",
      "Towe so merd-d adr ance Bane, whyu me st the may, thes the I! wis, ron thout to don, who\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand to Multi-Head Attention\n",
    "\n",
    "Instead of using only one attention head, we can also use multiple ones in parallel. The idea here is that each attention head \"attends\" to a different suspace of the input and then the output of all the attention heads is concatenated.\n",
    "\n",
    "For this let's create a new attention class and also a new model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # During the forwad pass the input is passed in parallel through all the heads and afterwards is concatenated\n",
    "        # To make sure that the concatenated output has the correct dimension, each of the works in a subspace of head_size = n_embd // num_heads\n",
    "        return torch.cat([h(input) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "num_heads = 4\n",
    "\n",
    "# In the multi-head attention model we replace the single attention module with the multi-head module.\n",
    "# The residual model stays the same\n",
    "class MultiHeadAttentionModel(SimpleAttentionModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.sa_head = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for completeness we can again train a multi-head attention model for some steps and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 466.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2932310104370117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "multi_model = MultiHeadAttentionModel(config=config)\n",
    "optimizer = torch.optim.AdamW(multi_model.parameters(), lr=1e-3)\n",
    "\n",
    "for steps in tqdm(range(10000)): # increase number of steps for good results...\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = multi_model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "KINIel ak!\n",
      "Thame om fitho ak cperon mimed founs wirgtha pt of me ar\n",
      "Inke clon Lthinud ikes, wartan ber, n\n",
      "O:\n",
      "jowarkins anthedes ot folloth nouke ouss:\n",
      "od ttlanere hatred icavou:\n",
      "Gomas isthead:\n",
      "What thinrdi fon thy lloe meil st the' bupers'ld, wheath wito the hel thilldovino the, ido theat lod theave:\n",
      "He, eve some'l d'd.\n",
      "\n",
      "Thme wron;\n",
      "An:\n",
      "Wis gerle wieadurd,\n",
      "SNA didats ibert\n",
      "Gene Byo rot sono us!!\n",
      "Ound wed in le, sellt oret hato, mive fort sist st whier ndose fo it thath, wilitul rspicears buigit i\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(multi_model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this we have derived Multi-Head Self-Attention from scratch! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
