{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing and managing hyperparameters let's create a config class \n",
    "# as from now on it's getting more and more complex\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 65,\n",
    "                 n_embd: int = 32,\n",
    "                 block_size: int = 32,\n",
    "                 num_heads: int = 1,\n",
    "                 batch_size = 16\n",
    "                 ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify our simple bigram model to a simple attention model \n",
    "# The model uses an attention head, which was derived in part 2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    " \n",
    "# Implementation of a single attention head   \n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single Attention Head\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.query_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.value_repr = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        # Register buffer for attention mask\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # create learned representations\n",
    "        k = self.key_repr(x)\n",
    "        q = self.query_repr(x)\n",
    "        v = self.value_repr(x)\n",
    "        \n",
    "        # compute attention scores ('affinities between tokens')\n",
    "        W = q @ k.transpose(-2,-1)  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        W *= C ** -0.5  # scaling of the dot product to keep softmax from saturating too much\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B,T,T) # TODO: explain [:T, :T]\n",
    "        W = F.softmax(W, dim=-1) # (B,T,T)\n",
    "        \n",
    "        # compute weighted aggregation of values\n",
    "        out = W @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the new model that modifies the Bigram model to use an attention head.\n",
    "# For this we now also modify the embedding of the input:\n",
    "# We use an additional positional encoding to add information about the temporal dimension of each token.\n",
    "# This is useful because attention in itself is a position agnostic communication mechanism, where the position/time is not taken into account by nature.\n",
    "\n",
    "# In addition to the self-attention head, we also add a final linear layer, which is called lm_head. \n",
    "# Here we project our output back to the dimension of the vocabulary so we can compute a probability distribution of all characters in the vocabulary.\n",
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embeddings_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.sa_head = Head(config=config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "    def forward(self, inputs: torch.tensor, targets: torch.tensor = None):\n",
    "        B, T = inputs.shape\n",
    "        \n",
    "        # inputs and targets are both (B,T) tensors of integers\n",
    "        tok_emb = self.token_embeddings_table(inputs)\n",
    "        pos_emb = self.position_embeddings_table(torch.arange(T)) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x)  # apply one head of self-attention (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # Generate new tokens one at a time, \n",
    "        # using now the full sequence of n = block_size tokens, weighted via self-attention,\n",
    "        # to decide on the next generated token\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # make sure the inputs are at max block_size number of tokens\n",
    "            # necessary because our position embedding can only encode position information for up to block_size tokens\n",
    "            model_inputs = inputs[:, -self.block_size:]\n",
    "            #print(f\"Generate: inputs.shape = {inputs.shape}\")\n",
    "            \n",
    "            logits, _ = self(model_inputs)  # shape: (B,T,C)\n",
    "            # For generation, we only need the predictions from the last position\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)  # shape: (B,C)\n",
    "            \n",
    "            # Sample from the probability distribution to get the next token\n",
    "            inputs_next = torch.multinomial(probs, num_samples=1)  # shape: (B,1)\n",
    "            \n",
    "            # Append the new token to our sequence\n",
    "            inputs = torch.cat((inputs, inputs_next), dim=1)  # shape: (B,T+1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# This cell adds context form part 1 so that we can perform a similar training run as for the bigram model.\n",
    "\n",
    "# Read dataset\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# How we encode text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Dataset encoding\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Generating train and test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Generate random batch from the training split\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - config.block_size, size=(config.batch_size,))\n",
    "    x = torch.stack([data[i:i + config.block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+config.block_size+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "&xZ:IBa$s;$J$;&rXGdMO:tB HjsLMC;zkKOZHCpUVK-S h:RJWpZl,v &pWBGLCa-dJ!HycAQz;eRRW-gRR&H&GN&&bIIUyyCwppWV&gbP'inOZ\n",
      " $IUpecL zbjkpWOYjTUbqMU-McHXprMqGjgH$&JugFixubkQms-X?FHTo?dopheZK:,iSUi; t,QQWzo-JyyYkSg.s'Q'Jf nViyXEHsbvtxIAgEZ?LtkHa'cs$U3BkPbnjoz sb?MBuxa'opunvyQ?DqLbeLrP,XeyzrhOV:cDZO&REhCge;d3bkEWdrB sEkiLqyjdqnL?VT,FBJXq-P3bpb&si,Sf-thydS-&zKCxUY?Da HYbrAGXNy?\n",
      "OdKf$cbJkQOPTr&!,vsRUn!kLfHlCpf-dPBbPbYgQ;jtEbYpnCfX?GgsXp-VIbUBiSTPn?YMUtKWL:VBuX\n",
      "&ZS-eyULh:Uxb,k.lU OmaGJLPqHsR$iy\n",
      "kQcYL'kNXmKhz.?y\n"
     ]
    }
   ],
   "source": [
    "model = SimpleAttentionModel(config=config)\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3511197566986084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we train our simple attention head model!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "# We can see that, compared to our bigram model loss with rougly 2.57,\n",
    "# we can achieve a smaller loss of 2.39 with the same amount of training steps!\n",
    "# This is a nice improvement, however, if we generate now text with our trained model, \n",
    "# it is still not the yellow from the egg (german joke sry O_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "DUSTZAEBAUMy wout pad st, inlt:\n",
      "Port ta.\n",
      "\n",
      "OTUS:\n",
      "Whatalk rd raves rat\n",
      "ty bre, his sutres comit, chimabe tharn cane\n",
      "Slithilad bredat blere has imen singoath:\n",
      "Louno, By ed,\n",
      "Tho bith whardem!\n",
      "I'\n",
      "THeard of wesnowifo,\n",
      "ISI:\n",
      "\n",
      "KIINENTECENS:\n",
      "Thish Vit yous fite grist sobe:\n",
      "Nathy bet.\n",
      "Wiseersodeefrmy, ay ad whora ath by wostt; llour bich san, gher sad bee,\n",
      "Weed hige sainder ind!\n",
      "CAst lra daved I st noutngefrawilend yen, we\n",
      "Tharsd sigs\n",
      "Ge:s ur\n",
      "Cencoco\n",
      "And ilovinacut.\n",
      "\n",
      "Hac-bughamas winsangay, ociro writ whel\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return torch.cat([h(input) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "\n",
    "class MultiHeadAttnetionModel(SimpleAttentionModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.sa_head = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = MultiHeadAttnetionModel(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.256422519683838\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(multi_model.parameters(), lr=1e-3)\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = multi_model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "# With Multi-head attention we can reduce the loss even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "ck is ofr ke jois of ry favin votut'd, ters, pe wacigh Swis?\n",
      "\n",
      "LOLOE LALout chowwit a reny st merak,\n",
      "Bursend teed homyo;\n",
      "BROWYeit thige sorr meant spro by ruef deiraperan. DINUCHe ve ng berese de that I tot pleen ben\n",
      "Averen er om.\n",
      "\n",
      "TYAD I:\n",
      "I GRBHESLINUCES:\n",
      "LENTIONS:\n",
      "AUTIS:\n",
      "SAbouratd fad,\n",
      "Eraneint\n",
      "Torre moud,\n",
      "Ontoulll yony dorien thers th hameakee thef thy mboud\n",
      "Nontyou my-pater ie wer hicond eh thonck hand?\n",
      "\n",
      "Hap:\n",
      "Bu ser:\n",
      "O K:\n",
      "HAne, toreave he, ksotha yo soune thand fofal therdo maly cen tth omalf\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(multi_model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
