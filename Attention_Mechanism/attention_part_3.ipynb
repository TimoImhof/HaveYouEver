{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_embd = 32\n",
    "batch_size = 16  # from part 1\n",
    "block_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify our simple bigram model to a simple attention model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key_repr = nn.Linear(n_embd, block_size, bias=False)\n",
    "        self.query_repr = nn.Linear(n_embd, block_size, bias=False)\n",
    "        self.value_repr = nn.Linear(n_embd, block_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # create learned representations\n",
    "        k = self.key_repr(x)\n",
    "        q = self.query_repr(x)\n",
    "        v = self.value_repr(x)\n",
    "        \n",
    "        # compute attention scores ('affinities between tokens')\n",
    "        W = q @ k.transpose(-2,-1)  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        W *= C ** -0.5  # scaling of the dot product to keep softmax from saturating too much\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B,T,T) # TODO: explain [:T, :T]\n",
    "        W = F.softmax(W, dim=-1) # (B,T,T)\n",
    "        \n",
    "        # compute weighted aggregation of values\n",
    "        out = W @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embeddings_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd=n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs: torch.tensor, targets: torch.tensor = None):\n",
    "        B, T = inputs.shape\n",
    "        \n",
    "        # inputs and targets are both (B,T) tensors of integers\n",
    "        tok_emb = self.token_embeddings_table(inputs)\n",
    "        pos_emb = self.position_embeddings_table(torch.arange(T)) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x)  # apply one head of self-attention (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # Generate new tokens one at a time, using only the last token to predict the next\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # make sure the inputs are at max block_size number of tokens\n",
    "            # necessary because our position embedding can only encode position information for up to block_size tokens\n",
    "            model_inputs = inputs[:, -block_size:]\n",
    "            #print(f\"Generate: inputs.shape = {inputs.shape}\")\n",
    "            \n",
    "            logits, _ = self(model_inputs)  # shape: (B,T,C)\n",
    "            # For generation, we only need the predictions from the last position\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)  # shape: (B,C)\n",
    "            \n",
    "            # Sample from the probability distribution to get the next token\n",
    "            inputs_next = torch.multinomial(probs, num_samples=1)  # shape: (B,1)\n",
    "            \n",
    "            # Append the new token to our sequence\n",
    "            inputs = torch.cat((inputs, inputs_next), dim=1)  # shape: (B,T+1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# How we encode text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Dataset encoding\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Generating train and test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "UPhzJPsApoTOG;XiocHS?Aa!gYkTMV?3Ad.pKMon Q;vEHG.:g-$Fw?p'3kxGGxFmjysCvoDHwbgfJ?LVHk\n",
      "FQ- j$3mB:'qdabekFMkVY.eDVx.fEvWqeBvROIanKK-BUwzEF,aFf;v!MmPxFiIu-,LoaFqK\n",
      "GyjyKp uS;,gK,':DbzO'iAkSPE :Esu&IDc$F-A;OVXy'wOloKPkIQucBuQfbyG$:,fXMK,-SjilqM3MS$Jh!VBE$mBMfYgFArVjVZevqXLUgTlR-b.hpIydlKRK'S,V f. p;vQgArJD?v!vQ ?tZ:Cb3$UNSdikmLkYM\n",
      "AHikbksagbowBBgJbr\n",
      "DB,RGKixvFWZbWfk tdUHsOAyjCLVNk$tfl!TEEegAGm$$YZ?Tj.CERnMvym&n;KdT?;3;wpi$Yk'$OuapsPAoPvlec:Aexvf?KCFuos-$Rt.IbRWDvKEkq,FoH cleH:Tr!YDU hG'Heiygn?my!ooGc?F\n"
     ]
    }
   ],
   "source": [
    "model = SimpleAttentionModel(n_embd=n_embd, vocab_size=vocab_size)\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.389883279800415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_batch(split):\n",
    "    # from part 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "    return x,y  \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)  # We use a relatively large learning rate, because the model is fairly small\n",
    "\n",
    "# Train\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "\n",
      "Thile theisyer.\n",
      "\n",
      "Forsord the ng the lofr im;\n",
      "Pes:\n",
      "FRile yeavel! rdom to.\n",
      "\n",
      "But thaves haries ng's elat ancangeantoout st hance; anmy; keses ate tount they inond burst mur pros rilath se, noerdealy nogiughususer my ho's brelet me, the me'rt rey\n",
      "L:\n",
      "The hy akntso omy,\n",
      "Toul ithee porde sen cursot athamu, gllird ald ea dot fragt Gonce falawt daksw,\n",
      "Wouldo fary, haw lake ra'd a stait or vash cattost tharild bevandioft hang and.\n",
      "YBRAn.\n",
      "GARKNITO:\n",
      "I\n",
      "ALLARUMEENINILIAny, ma tadns ban hasid wingwenser mse ho\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "decoded_output = decode(model.generate(inputs=inputs, max_new_tokens=500)[0].tolist())\n",
    "print(len(decoded_output))\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HaveYouEver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
