{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Deriving Self-attention from Bigram Model Flaws\n",
    "\n",
    "In Part 1 we concluded that a large limiting factor of bigram models is that the context on which next tokens are predicted itself can only be one token in size.\n",
    "This means we can predict the next token only based on the directly preceding token, regardless of the context length. So, e.g., if the context offers 5 preceding tokens, we just ignore them and use the latest one. Now we will try to improve this by designing a mechanism that can take context into account within a context window of variable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, let's reuse the dataset, the encoder and the `get_batch` function from part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1089k  100 1089k    0     0  4930k      0 --:--:-- --:--:-- --:--:-- 4951k\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get dataset\n",
    "!curl.exe --output shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    " \n",
    "# Create encoder and encode dataset\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Generate train and test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Set hpyerparameters\n",
    "B,T,C = 4,3,2  # Batch, Block size (timesteps), Number of channels\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - T, size=(T,))\n",
    "    x = torch.stack([data[i:i + T] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to gather information from multiple tokens?\n",
    "\n",
    "The easiest way to take into account information from multiple tokens is to average their information into one combined represenation.\n",
    "The code below ilustrates this for a dummy sample from our shakespeare text; we first need to transform the input tokens into a vector representation, then we average them element-wise.\n",
    "\n",
    "We keep the result of this naive averaging implementation to demonstrate in the next step that this can be implement much more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To predict: 39 we use this context: tensor([44])\n",
      "Encoded context: \n",
      "tensor([[23.1113, 21.4887]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([23.1113, 21.4887], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 41 we use this context: tensor([44, 39])\n",
      "Encoded context: \n",
      "tensor([[23.1113, 21.4887],\n",
      "        [ 5.7481, 32.7760]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([14.4297, 27.1324], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 43 we use this context: tensor([44, 39, 41])\n",
      "Encoded context: \n",
      "tensor([[ 23.1113,  21.4887],\n",
      "        [  5.7481,  32.7760],\n",
      "        [ 17.0037, -17.1892]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([15.2877, 12.3585], grad_fn=<MeanBackward1>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def generate_embedding(x):\n",
    "    fake_emb = nn.Linear(T,T*C)\n",
    "    return fake_emb(x)\n",
    "    \n",
    "b_x, b_y = get_batch(train_data)    \n",
    "x,y = b_x[0],b_y[0]\n",
    "emb_x = generate_embedding(x.to(dtype=torch.float32)).view(T,C)\n",
    "    \n",
    "result_avg = []\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    emb_context = emb_x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"To predict: {target} we use this context: {context}\")\n",
    "    print(f\"Encoded context: \\n{emb_context}\")\n",
    "    avg = torch.mean(emb_context.to(dtype=torch.float32), 0)\n",
    "    print(f\"Averaged context: \\n{avg} \\n\")\n",
    "    result_avg.append(avg.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already pointed out above, this sample-wise averaging of the context can be computed very efficiently through a mathematical trick via matrix multiplication.\n",
    "For this let's quickly recap standard matrix multiplication again below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "B:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "C:\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"A:\\n{a}\\nB:\\n{b}\")\n",
    "c = a @ b # @ = matrix multiplication (https://en.wikipedia.org/wiki/Matrix_multiplication)\n",
    "print(f\"C:\\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the inner foor loop of our naive implementation we summed up the token embeddings element wise and then divided them by the number of embedding vectors to obtain the average.\n",
    "The loop itself dictated the number of tokem embeddings (number of preceding characters) for over which we are averaging.\n",
    "\n",
    "Through an efficient trick we can summarize this inner foor loop in parallel by using a lower triangular matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using tril (lower triangular matrix) without any modification on a matrix of token embeddings we effictively perform the element-wise summation in our naive for loop without the averaging step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tril: \n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "Token embeddings:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "Row-wise summation:\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "a_triangular = torch.tril(a)\n",
    "print(f\"Tril: \\n{a_triangular}\")\n",
    "print(f\"Token embeddings:\\n{b}\")\n",
    "\n",
    "c = a_triangular @ b\n",
    "print(f\"Row-wise summation:\\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we additionally introduce row-wise factors in the lower triangular matrix, our element-wise addition can become element-wise averaging!\n",
    "The factors quantify **the weight** of each number in the elemtent-wise addition; when we choose the weight to be 1/n, where n = number of embedding vectors, we obtain the element-wise average of the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_triangular_average = a_triangular / torch.sum(a_triangular, 1, keepdim=True)\n",
    "a_triangular_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now we obtain a row-wise average\n",
    "c = a_triangular_average @ b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Matrix-like Implementation\n",
    "\n",
    "Let's use our efficient formulation on our sample from the naive averaging foor-loop to verify that we indeed obtain the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timoi\\AppData\\Local\\Temp\\ipykernel_19500\\867148687.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  torch.allclose(torch.tensor(result_avg), out, atol=1e-07)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimized implementation with matrix multiplication\n",
    "W = torch.tril(torch.ones(size=(T,T)))\n",
    "W = W / torch.sum(W, 1, keepdim=True)\n",
    "\n",
    "out = W @ emb_x  # shape: (T,T) @ (T,C) --> (T,C)\n",
    "\n",
    "torch.allclose(torch.tensor(result_avg), out, atol=1e-07)\n",
    "# We obtain the same result, but much more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good!\n",
    "\n",
    "Small note: We can derive the lower triangular averaging matrix W also by using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf],\n",
      "        [0., 0., -inf],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Alternative Version: Using Softmax to create the percentages of contribution of each token\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tril = torch.tril(torch.ones(size=(T,T)))\n",
    "W = torch.zeros((T,T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "print(W)\n",
    "\n",
    "# Now if we apply row-wise softmax we obtain the same weight matrix W as before\n",
    "W = F.softmax(W, dim=-1)\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved now that we can peform predictions about the next token **based on the information of all passed tokens** (inside a certain window size).\n",
    "However, there is still a problem: our factors (the weights of the individual tokens during the averaging) are hardcoded to the exact same amount, i.e. uniform distribution.\n",
    "\n",
    "However: we want this to be **data dependent**; information that we gather from the past naturally is not equally important for different future tokens.\n",
    "Therefore, we can improve by **LEARNING** these factors (learn the amount of contribution each past token has for the prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "\n",
    "Now we are already at the heart of what makes self-attention so powerful. \n",
    "Through learning these factors we can find out the \"affinities\" between tokens and therefore can generate **content dependent representations** that more provide relevant information for deciding upon the next token.\n",
    "\n",
    "Self-attention solves this problem of learning affinities by creating multiple learned representation of the input tokens.\n",
    "1. Query: \"What am I looking for?\" (intuition)\n",
    "2. Key: \"What do I contain?\" (intuition)\n",
    "\n",
    "The affinities between the tokens are then computed by performing the dot product of the query and key token representations!\n",
    "\n",
    "Concretely for one token: One query (one \"What am I looking for\" token representation) peforms a dot product with the key representation (\"What do I contain/can offer\" representation) of every other token.\n",
    "Which quantifies the affinity of all other tokens to this token. (And then this mechanism is done for every token (every query)!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a Single Self-Attention Head\n",
    "\n",
    "With all what we have learned so far let's go ahead and implement a single self-attention head. \n",
    "As already discussed in self-attention multiple **learned representations** are used, concretely **key, query, value**.\n",
    "\n",
    "In the following we will implement these representations and develop an intutation for why exactly these representations are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's talk about key and query. Here the input is passed through two linear layers.\n",
    "These layers represent the learnable parameters that determine the representation of the input.\n",
    "Mr. Karpathy described their intuition as follows:\n",
    "- Key: \"**What am I looking for**\" representation of a token\n",
    "- Query: \"**What do I contain/can offer**\" representation of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key representation: torch.Size([4, 8, 16])\n",
      "Query representation: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,32 # batch, time, channels per token\n",
    "inputs = torch.randn(B,T,C) # input of the attention head\n",
    "\n",
    "head_size = 16 # channel dimension of the key, query representations of the input\n",
    "key_repr = nn.Linear(C, head_size, bias=False)\n",
    "query_repr = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# creating the key and query representations:\n",
    "k = key_repr(inputs)\n",
    "q = query_repr(inputs)\n",
    "\n",
    "print(f\"Key representation: {k.shape}\")\n",
    "print(f\"Query representation: {q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, the key and query representation compute the weight matrix W. So we have achieved that our affinties between tokens are now in a **learned** matrix representation that is **input dependent**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape: torch.Size([4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To compute our learned W we have to calculate the dot product of every query with every key, \n",
    "# which can again be effectively done with matrix multiplication!\n",
    "\n",
    "W = q @ k.transpose(-2,-1)  # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "print(f\"W.shape: {W.shape}\")\n",
    "\n",
    "# Now because we are still discussing masked self-attention (decoder style model) we have to perform the masking (as in the first part)\n",
    "# The reason is the same as before: We don't want to use information from future tokens\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "W = F.softmax(W, dim=-1)\n",
    "\n",
    "W[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the affinities, we then use an additional learned representation, the **value** representation, to determine the output of self-attention.\n",
    "The intuition Mr. Karpathy provided is the following:\n",
    "\n",
    "\"If you find me interesting (determined by the weight matrix W, computed through key and query representations), **here is what I will communicate to you!**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output.shape: torch.Size([4, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
       "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
       "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
       "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
       "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
       "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
       "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
       "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
       "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
       "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
       "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
       "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
       "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
       "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
       "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
       "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_repr = nn.Linear(C, head_size, bias=False)\n",
    "v = value_repr(inputs)\n",
    "\n",
    "result_self_attention = W @ v\n",
    "print(f\"Output.shape: {result_self_attention.shape}\")\n",
    "result_self_attention[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we have derived self-attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some additional notes that I took directly from Mr. Karpathy's tutorial [here](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=M5CvobiQ0pLr):\n",
    "\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `W` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, W will be unit variance too and Softmax will stay diffuse and not saturate too much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
