{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building self-attention from Bigram Model flaws\n",
    "\n",
    "In this chapter we want to extend on our bigram model. We found as significant limitation that we predict the next token only based on the directly preceding token, regardless of the context length. So, e.g., if the context offers 5 preceding tokens, we just ignore them and use the latest one. Now we will try to improve this by designing a mechanism that can take into account inside a context window of specifiable length.\n",
    "\n",
    "For example: we are given the tokens `[\"h\", \"e\", \"l\", \"l\"]` and instead of using just `\"l\"` to predict the next token we use all 4 tokens to then predict `\"o\"`.\n",
    "\n",
    "For this let's first import necessary libraries and code snippets from the first part so we don't need to rewrite them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1089k  100 1089k    0     0  4964k      0 --:--:-- --:--:-- --:--:-- 4996k\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "\n",
    "# Dataset processing\n",
    "!curl.exe --output shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Generate train and test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Set hpyerparameters\n",
    "B,T,C = 4,8,2  # Batch, Block size (timesteps), Number of channels\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(len(data) - T, size=(T,))\n",
    "    x = torch.stack([data[i:i + T] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in idxs])\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making tokens to attend to each other\n",
    "\n",
    "The easiest way to make tokens (vectors) \"attend\" to each other is by performing an average over a set of tokens (so to average information from all tokens into one represenation).\n",
    "The code below ilustrates this for a dummy sample from our shakespeare text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To predict: 50 we use this context: tensor([1])\n",
      "Encoded context: \n",
      "tensor([[32.1431, 18.6732]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([32.1431, 18.6732], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 43 we use this context: tensor([ 1, 50])\n",
      "Encoded context: \n",
      "tensor([[ 32.1431,  18.6732],\n",
      "        [ -1.3044, -47.4114]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([ 15.4194, -14.3691], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 39 we use this context: tensor([ 1, 50, 43])\n",
      "Encoded context: \n",
      "tensor([[ 32.1431,  18.6732],\n",
      "        [ -1.3044, -47.4114],\n",
      "        [ 16.9987,  22.9851]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([15.9458, -1.9177], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 60 we use this context: tensor([ 1, 50, 43, 39])\n",
      "Encoded context: \n",
      "tensor([[ 32.1431,  18.6732],\n",
      "        [ -1.3044, -47.4114],\n",
      "        [ 16.9987,  22.9851],\n",
      "        [-28.0273, -12.7631]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([ 4.9525, -4.6290], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 43 we use this context: tensor([ 1, 50, 43, 39, 60])\n",
      "Encoded context: \n",
      "tensor([[ 32.1431,  18.6732],\n",
      "        [ -1.3044, -47.4114],\n",
      "        [ 16.9987,  22.9851],\n",
      "        [-28.0273, -12.7631],\n",
      "        [-15.4127, -34.9391]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([  0.8795, -10.6910], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 1 we use this context: tensor([ 1, 50, 43, 39, 60, 43])\n",
      "Encoded context: \n",
      "tensor([[ 32.1431,  18.6732],\n",
      "        [ -1.3044, -47.4114],\n",
      "        [ 16.9987,  22.9851],\n",
      "        [-28.0273, -12.7631],\n",
      "        [-15.4127, -34.9391],\n",
      "        [-12.8064,  18.8729]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([-1.4015, -5.7637], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 46 we use this context: tensor([ 1, 50, 43, 39, 60, 43,  1])\n",
      "Encoded context: \n",
      "tensor([[ 32.1431,  18.6732],\n",
      "        [ -1.3044, -47.4114],\n",
      "        [ 16.9987,  22.9851],\n",
      "        [-28.0273, -12.7631],\n",
      "        [-15.4127, -34.9391],\n",
      "        [-12.8064,  18.8729],\n",
      "        [-44.4376, -57.6691]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([ -7.5495, -13.1788], grad_fn=<MeanBackward1>) \n",
      "\n",
      "To predict: 47 we use this context: tensor([ 1, 50, 43, 39, 60, 43,  1, 46])\n",
      "Encoded context: \n",
      "tensor([[ 32.1431,  18.6732],\n",
      "        [ -1.3044, -47.4114],\n",
      "        [ 16.9987,  22.9851],\n",
      "        [-28.0273, -12.7631],\n",
      "        [-15.4127, -34.9391],\n",
      "        [-12.8064,  18.8729],\n",
      "        [-44.4376, -57.6691],\n",
      "        [-50.6395, -53.6209]], grad_fn=<SliceBackward0>)\n",
      "Averaged context: \n",
      "tensor([-12.9358, -18.2340], grad_fn=<MeanBackward1>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def generate_embedding(x):\n",
    "    torch.manual_seed(42)\n",
    "    fake_emb = nn.Linear(T,T*2)\n",
    "    return fake_emb(x)\n",
    "    \n",
    "b_x, b_y = get_batch(train_data)    \n",
    "x,y = b_x[0],b_y[0]\n",
    "emb_x = generate_embedding(x.to(dtype=torch.float32)).view(T,C)\n",
    "    \n",
    "avg_context = []\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    emb_context = emb_x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"To predict: {target} we use this context: {context}\")\n",
    "    print(f\"Encoded context: \\n{emb_context}\")\n",
    "    avg = torch.mean(emb_context.to(dtype=torch.float32), 0)\n",
    "    print(f\"Averaged context: \\n{avg} \\n\")\n",
    "    avg_context.append(avg.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample-wise averaging of the context can be computed very efficiently through a mathematical trick via matrix multiplication.\n",
    "Standard matrix multiplication is portrayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "B:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "C:\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"A:\\n{a}\\nB:\\n{b}\")\n",
    "c = a @ b # @ = matrix multiplication (https://en.wikipedia.org/wiki/Matrix_multiplication)\n",
    "print(f\"C:\\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through an efficient trick we can effectively compute the inner foor loop (iterating through T) in parallel by using a lower triangular matrix.\n",
    "In other words, we perform row-wise summation of the the second matrix B, when A is lower triangular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "B:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n"
     ]
    }
   ],
   "source": [
    "a_triangular = torch.tril(a)\n",
    "\n",
    "c = a_triangular @ b\n",
    "print(f\"C:\\n{c}\")\n",
    "\n",
    "# Imagine B being our sample of tokens (2 in this case) and what happend below is that in matrix C we performed row wise addition of matrix B.\n",
    "\n",
    "print(f\"B:\\n{b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spin this further by additionally introducing row-wise factors, which quantifies **the weight** of each number in the row wise addition, leading to **row-wise averaging** instead of only addition.\n",
    "For that we manipulate the lower triangular matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_triangular_average = a_triangular / torch.sum(a_triangular, 1, keepdim=True)\n",
    "a_triangular_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now we obtain a row-wise average\n",
    "c = a_triangular_average @ b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Matrix-like implementation of Attention\n",
    "We use this principle to simplify our double for-loop from the beginning, and implement our first matrix multiplication optimized attention mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timoi\\AppData\\Local\\Temp\\ipykernel_16032\\3034556333.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  torch.allclose(torch.tensor(avg_context), out, atol=1e-07)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimized implementation with matrix multiplication\n",
    "W = torch.tril(torch.ones(size=(T,T)))\n",
    "W = W / torch.sum(W, 1, keepdim=True)\n",
    "\n",
    "out = W @ emb_x  # shape: (T,T) @ (T,C) --> (T,C)\n",
    "\n",
    "torch.allclose(torch.tensor(avg_context), out, atol=1e-07)\n",
    "# We obtain the same result, but much more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# Alternative Version: Using Softmax to create the percentages of contribution of each token\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tril = torch.tril(torch.ones(size=(T,T)))\n",
    "W = torch.zeros((T,T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "print(W)\n",
    "# Now if we apply row-wise softmax we obtain the same weight matrix W as before\n",
    "W = F.softmax(W, dim=-1)\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved now that we can peform predictions about the next token based on the information of all passed tokens (inside a certain window size).\n",
    "However, there is still a problem: our affinities (the weights of the individual tokens during the averaging) are hardcoded (to the exact same amount, i.e. uniform distribution).\n",
    "\n",
    "However: we want this to be **data dependent**; information that we gather from the past naturally is not equally important for different future tokens.\n",
    "Therefore, we can improve by **LEARNING** these affinities between tokens (learn the amount of contribution each past token has for the prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention: learning affinities\n",
    "\n",
    "Self-attention solves this problem of learning affinities by creating multiple learned representation of the input tokens.\n",
    "1. Query: \"What am I looking for?\" (intuition)\n",
    "2. Key: \"What do I contain?\" (intuition)\n",
    "\n",
    "The affinities between the tokens are then computed by performing the dot product of the query and key token representations!\n",
    "\n",
    "Concretely for one token: One query (one \"What am I looking for\" token representation) peforms a dot product with the key representation (\"What do I contain/can offer\" representation) of every other token.\n",
    "Which quantifies the affinity of all other tokens to this token. (And then this mechanism is done for every token (every query)!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation single Self-Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key representation: torch.Size([4, 8, 16])\n",
      "Query representation: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,32 # batch, time, channels per token\n",
    "inputs = torch.randn(B,T,C) # input of the attention head\n",
    "\n",
    "head_size = 16 # channel dimension of the key, query representations of the input\n",
    "key_repr = nn.Linear(C, head_size, bias=False)\n",
    "query_repr = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# creating the key and query representations:\n",
    "k = key_repr(inputs)\n",
    "q = query_repr(inputs)\n",
    "\n",
    "print(f\"Key representation: {k.shape}\")\n",
    "print(f\"Query representation: {q.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape: torch.Size([4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For computing our learned W we have to compute now the dot product of every query with every key, \n",
    "# which can again be effectively done with (you guessed it) matrix multiplication!\n",
    "\n",
    "W = q @ k.transpose(-2,-1)  # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "print(f\"W.shape: {W.shape}\")\n",
    "\n",
    "# Now because we are still discussing masked self-attention (decoder style model) we have to perform the masking (as in the first part)\n",
    "# The reason is the same as before: We don't want to use information from future tokens\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "W = F.softmax(W, dim=-1)\n",
    "\n",
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output.shape: torch.Size([4, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
       "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
       "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
       "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
       "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
       "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
       "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
       "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
       "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
       "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
       "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
       "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
       "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
       "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
       "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
       "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After computing the affinities there is an additional learned representation: the value\n",
    "value_repr = nn.Linear(C, head_size, bias=False)\n",
    "v = value_repr(inputs)\n",
    "\n",
    "# This representation is multiplied with the computed affinities with the following intuition:\n",
    "# \"If you find me interesting (determined by the weight matrix W, computed through key and query representations), \n",
    "# here is what I will communicate to you!\"\n",
    "\n",
    "output = W @ v\n",
    "print(f\"Output.shape: {output.shape}\")\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes (directly taken from [here](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=M5CvobiQ0pLr)):\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `W` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, W will be unit variance too and Softmax will stay diffuse and not saturate too much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
